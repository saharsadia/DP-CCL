# -*- coding: utf-8 -*-
"""DP-CCL for Github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11tpviGAOVxtRkC-pJp4rZhGtPiLLpp53
"""

from google.colab import drive
drive.mount('/content/drive')

"""#configuration"""

import os
import sys
import time
import torch
import random
import logging
import argparse
from datetime import datetime

import logging

''' Base '''
data_dir='/content/drive/MyDrive/All_Data/AST_Tokens/sequence_and_label'
dataset= 'velocity_1.5-1.6'
model_name='microsoft/codebert-base'
method='scl'
''' Optimization '''
train_batch_size=16
test_batch_size=64
num_epoch=5
lr=1e-5
decay=0.01
alpha=0.5
temp=0.1

''' Environment '''
backend=False
timestamp='{:.0f}{:03}'.format(time.time(), random.randint(0, 999))
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
num_classes = 2
log_name = '/content/drive/MyDrive/Proposed_BaseLine/BERT_SCL_LR/Models_HC/{}_{}_{}_{}.log'.format(dataset, model_name, method, datetime.now().strftime('%Y-%m-%d_%H-%M-%S')[2:])
if not os.path.exists('logs'):
  os.mkdir('logs')
logger = logging.getLogger()
logger.setLevel(logging.INFO)
logger.addHandler(logging.StreamHandler(sys.stdout))
logger.addHandler(logging.FileHandler(os.path.join('logs', log_name)))

"""#preprocess HC dataset"""

from sklearn.preprocessing import StandardScaler
def pre_process(X_data):
  sc = StandardScaler()
  X_data =  X_data.split(',')
  X_data =X_data[0:promise_dim]
  X_data = [[float(x)] for x in X_data]
  #X_data = [float(x) for x in X_data]
  sc.fit(X_data)
  X_data=sc.transform(X_data)
  X_data = [x[0] for x in X_data]
  #print(X_data)
  return X_data

"""#data_utils"""

import os
import json
import torch
import random
from functools import partial
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):

    def __init__(self, raw_data, label_dict, tokenizer, model_name, method):

        sep_token = ['</s>']
        dataset = list()

        for data in raw_data:
          data['sequence'] = data['sequence'].replace("_", " ")
          tokens = data['sequence'].lower().split(' ')
          label_id = label_dict[data['bug']]
          prom_data = data['promise']
          prom_data = pre_process(prom_data)
          dataset.append((sep_token + tokens, prom_data, label_id))

        self._dataset = dataset

    def __getitem__(self, index):
        return self._dataset[index]

    def __len__(self):
        return len(self._dataset)


def my_collate(batch, tokenizer, method, num_classes):
    tokens,promise_data, label_ids = map(list, zip(*batch))
    text_ids = tokenizer(tokens,
                         padding=True,
                         truncation=True,
                         max_length=512,
                         is_split_into_words=True,
                         add_special_tokens=True,
                         return_tensors='pt')

    return text_ids,promise_data, torch.tensor(label_ids)

from torch.utils.data import WeightedRandomSampler
def weight_sampler(trainset):
  count_true=0
  count_neg=0
  for i in range(len(trainset)):
    if trainset[i][2]==0:
      count_true += 1
    else:
      count_neg +=1
  #print(count_true,count_neg)
  counts=[count_true,count_neg]
  class_weights=[sum(counts)/c for c in counts]
  class_weights_all = [] #torch.tensor(len(trainset))
  for i in range(len(trainset)):
    if trainset[i][2]==0:
      class_weights_all.append(class_weights[0])
    else:
      class_weights_all.append(class_weights[1])
  target_tensor = torch.Tensor(class_weights_all)
  weighted_sampler = WeightedRandomSampler(
    weights=class_weights_all,
    num_samples=len(class_weights_all),
    replacement=True)

  return weighted_sampler

def load_data(dataset, data_dir, tokenizer, train_batch_size, test_batch_size, model_name, method, workers):
    if dataset == 'ant_camel':
        train_data = json.load(open(os.path.join(data_dir, 'ant_1.5.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'camel_1.2.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'ant_1.5-1.6':
        train_data = json.load(open(os.path.join(data_dir, 'ant_1.5.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'ant_1.6.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'ant_1.6-1.7':
        train_data = json.load(open(os.path.join(data_dir, 'ant_1.6.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'ant_1.7.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'camel_1.2-1.4':
        train_data = json.load(open(os.path.join(data_dir,'camel_1.2.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'camel_1.4.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'camel_1.4-1.6':
        train_data = json.load(open(os.path.join(data_dir, 'camel_1.4.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'camel_1.6.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'ivy_1.4-2.0':
        train_data = json.load(open(os.path.join(data_dir, 'ivy_1.4.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'ivy_2.0.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'jedit_3.2-4.0':
        train_data = json.load(open(os.path.join(data_dir, 'jedit_3.2.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'jedit_4.0.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'jedit_4.0-4.1':
        train_data = json.load(open(os.path.join(data_dir, 'jedit_4.0.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'jedit_4.1.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'log4j_1.0-1.1':
        train_data = json.load(open(os.path.join(data_dir, 'log4j_1.0.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'log4j_1.1.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'lucene_2.0-2.2':
        train_data = json.load(open(os.path.join(data_dir, 'lucene_2.0.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'lucene_2.2.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'lucene_2.2-2.4':
        train_data = json.load(open(os.path.join(data_dir, 'lucene_2.2.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'lucene_2.4.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'poi_1.5-2.5':
        train_data = json.load(open(os.path.join(data_dir, 'poi_1.5.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'poi_2.5.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'poi_2.5-3.0':
        train_data = json.load(open(os.path.join(data_dir, 'poi_2.5.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'poi_3.0.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'synapse_1.0-1.1':
        train_data = json.load(open(os.path.join(data_dir, 'synapse_1.0.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'synapse_1.1.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'synapse_1.1-1.2':
        train_data = json.load(open(os.path.join(data_dir, 'synapse_1.1.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'synapse_1.2.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'xalan_2.4-2.5':
        train_data = json.load(open(os.path.join(data_dir, 'xalan_2.4.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'xalan_2.5.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'xerces_1.2-1.3':
        train_data = json.load(open(os.path.join(data_dir, 'xerces_1.2.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'xerces_1.3.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'xerces_1.3-1.4.4':
        train_data = json.load(open(os.path.join(data_dir, 'xerces_1.3.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'xerces_1.4.4.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    elif dataset == 'velocity_1.5-1.6':
        train_data = json.load(open(os.path.join(data_dir, 'velocity_1.5.json'), 'r', encoding='utf-8'))
        test_data = json.load(open(os.path.join(data_dir, 'velocity_1.6.json'), 'r', encoding='utf-8'))
        label_dict = {'positive': 1, 'negative': 0}
    else:
        raise ValueError('unknown dataset')

    #trainset = MyDataset(train, label_dict, tokenizer, model_name, method)
    trainset = MyDataset(train_data,label_dict, tokenizer, model_name, method)
    testset = MyDataset(test_data, label_dict, tokenizer, model_name, method)

    testset, validset = train_test_split(testset, test_size=0.2, random_state=42)

    weighted_sampler=weight_sampler(trainset)

    train_dataloader = DataLoader(trainset, train_batch_size, shuffle=False, num_workers=workers,
                                  collate_fn=partial(my_collate, tokenizer=tokenizer,method='dualcl', num_classes=2),sampler=weighted_sampler, pin_memory=True)
    valid_dataloader = DataLoader(validset, test_batch_size, shuffle=False, num_workers=workers,
                                 collate_fn=partial(my_collate, tokenizer=tokenizer,method='dualcl', num_classes=2), pin_memory=True)
    test_dataloader = DataLoader(testset, test_batch_size, shuffle=False, num_workers=workers,
                                 collate_fn=partial(my_collate, tokenizer=tokenizer,method='dualcl', num_classes=2), pin_memory=True)

    return train_dataloader, test_dataloader, valid_dataloader

"""#loss function"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class CELoss(nn.Module):

    def __init__(self):
        super().__init__()
        self.xent_loss = nn.CrossEntropyLoss(torch.Tensor([0.75, 0.44]))

    def forward(self, outputs, targets):
        return self.xent_loss(outputs['predicts'], targets)

class SupConLoss(nn.Module):

    def __init__(self, alpha, temp):
        super().__init__()
        self.xent_loss = nn.CrossEntropyLoss()
        self.alpha = alpha
        self.temp = temp

    def nt_xent_loss(self, anchor, target, labels):
        with torch.no_grad():
            labels = labels.unsqueeze(-1)
            mask = torch.eq(labels, labels.transpose(0, 1))
            # delete diag elem
            mask = mask ^ torch.diag_embed(torch.diag(mask))
        # compute logits
        anchor_dot_target = torch.einsum('bd,cd->bc', anchor, target) / self.temp
        # delete diag elem
        anchor_dot_target = anchor_dot_target - torch.diag_embed(torch.diag(anchor_dot_target))
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_target, dim=1, keepdim=True)
        logits = anchor_dot_target - logits_max.detach()
        # compute log prob
        exp_logits = torch.exp(logits)
        # mask out positives
        logits = logits * mask
        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)
        # in case that mask.sum(1) is zero
        mask_sum = mask.sum(dim=1)
        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)
        # compute log-likelihood
        pos_logits = (mask * log_prob).sum(dim=1) / mask_sum.detach()
        loss = -1 * pos_logits.mean()
        return loss

    def forward(self, outputs, targets):
        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)
        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)
        cl_loss = self.alpha * self.nt_xent_loss(normed_cls_feats, normed_cls_feats, targets)
        return ce_loss + cl_loss

"""#model"""

import torch
import torch.nn as nn

class Transformer(nn.Module):

    def __init__(self, base_model, num_classes, method, promise_dim):
        super().__init__()
        self.base_model = base_model
        self.num_classes = num_classes
        self.method = method
        #self.l2_lambda = 0.001
        #self.l2_regularization = torch.tensor(0., requires_grad=True)
        self.linear = nn.Linear(base_model.config.hidden_size, 300)
        self.pred_class = nn.Linear(300+promise_dim, num_classes)
        self.dropout = nn.Dropout(0.5)
        for param in base_model.parameters():
            param.requires_grad_(True)

    def forward(self,promise, *args, **kwargs):
        raw_outputs = self.base_model(*args, **kwargs)
        hiddens = raw_outputs.last_hidden_state

        #cls_feats = torch.sum(hiddens, 1)  sum of all hidden
        cls_feats = torch.mean(hiddens, 1)  #average of all hidden


        label_feats = None
        predicts = self.linear(self.dropout(cls_feats))
        predicts = torch.cat((predicts, promise), 1)
        predicts = self.pred_class(self.dropout(predicts))
        predicts = torch.sigmoid(predicts)

        outputs = {
            'predicts': predicts,
            'cls_feats': cls_feats,
            'label_feats': label_feats
        }
        return outputs

"""#main Code"""

pip install transformers

pip install torchmetrics

import torch
from tqdm import tqdm
from transformers import logging, AutoTokenizer, AutoModel
from transformers import logging, AutoTokenizer, AutoModel
from torchmetrics import F1Score
from sklearn.metrics import accuracy_score
import numpy as np
from numpy import vstack
from torchmetrics.classification import BinaryF1Score
from torchmetrics import Accuracy

from sklearn.metrics import f1_score
from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix
from torchmetrics.classification import BinaryConfusionMatrix
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, classification_report

def ppv(tn, fp, fn, tp):
  return tp / (tp + fp)

def tpr(tn, fp, fn, tp):
  return tp / (tp + fn)

def f1(tp, fn, fp, tn):
  p = ppv(tn, fp, fn, tp)
  r = tpr(tn, fp, fn, tp)

  return (2*p*r) / (p + r)

from transformers import BertModel

PATH = '/content/drive/MyDrive/Proposed_BaseLine/BERT_SCL_LR/Models_HC/'

from transformers import LongformerTokenizer, LongformerModel

class Instructor:

    def __init__(self, logger):
        self.logger = logger
        self.logger.info('> creating model {}'.format(model_name))


        self.tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base', add_prefix_space=True)
        base_model= AutoModel.from_pretrained('microsoft/codebert-base')
        self.model = Transformer(base_model, num_classes, method, promise_dim)
        self.model.to(device)
        if device.type == 'cuda':
            self.logger.info('> cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device.index)))
        self._print_args()

    def _print_args(self):
        self.logger.info('> training arguments:')


    def _train(self, dataloader, criterion, optimizer):
        train_loss, n_correct, n_train = 0, 0, 0

        n=0
        preds, actuals= list(), list()

        self.model.train()
        for inputs,promise, targets in tqdm(dataloader, disable=backend, ascii=' >='):
            inputs = {k: v.to(device) for k, v in inputs.items()}
            promise = torch.tensor(promise).to(device)
            targets = targets.to(device)
            outputs = self.model(promise.float(), **inputs)
            loss = criterion(outputs, targets)

            #Replaces pow(2.0) with abs() for L1 regularization
            l2_lambda = 0.001
            l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())
            loss = loss + l2_lambda * l2_norm

            torch.cuda.empty_cache()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            torch.cuda.empty_cache()

            train_loss += loss.item() * targets.size(0)
            n_correct += (torch.argmax(outputs['predicts'], -1) == targets).sum().item()
            n_train += targets.size(0)

            prediction=torch.argmax(outputs['predicts'], -1)
            #targ = torch.tensor(targets)
            #preds = torch.tensor(prediction)


            #confusion matrix
            metric_bin = BinaryConfusionMatrix().to(device)
            #print("\n")
            #print(metric_bin(prediction,targets))

            n +=1


        return train_loss / n_train, n_correct / n_train

    def _test(self, dataloader, criterion):
        test_loss, n_correct, n_test = 0, 0, 0
        self.model.eval()
        n=0

        tru_lbl=[]
        pr_lbl=[]

        out=[]
        prd=[]

        with torch.no_grad():
            for inputs, promise, targets in tqdm(dataloader, disable=backend, ascii=' >='):
                inputs = {k: v.to(device) for k, v in inputs.items()}
                promise = torch.tensor(promise).to(device)
                targets = targets.to(device)
                outputs = self.model(promise.float(), **inputs)
                loss = criterion(outputs, targets)
                test_loss += loss.item() * targets.size(0)
                n_correct += (torch.argmax(outputs['predicts'], -1) == targets).sum().item()
                n_test += targets.size(0)

                n +=1

                prediction=torch.argmax(outputs['predicts'], -1)
                #confusion matrix
                metric_bin = BinaryConfusionMatrix().to(device)
                #print("\n")
                #print(metric_bin(prediction,targets))

                pr_lbl.extend(prediction.detach().cpu().numpy())
                tru_lbl.extend(targets.detach().cpu().numpy())

                out.extend(outputs['predicts'])
                prd.extend(targets)

        tn, fp, fn, tp = confusion_matrix(tru_lbl, pr_lbl).ravel()
        #f1_pos =  f1(tn, fp, fn, tp)
        #tp, fn, fp, tn = confusion_matrix(tru_lbl, pr_lbl).ravel()
        f1_neg= f1(tp, fn, fp, tn)

        #print("code is here")
        #loss = crtn(torch.tensor(out), torch.tensor(prd))
        print(classification_report(tru_lbl, pr_lbl))
        print(confusion_matrix(tru_lbl, pr_lbl))
        #Accuracy score
        #print("ACC: ",accuracy_score(pr_lbl,tru_lbl))
        #print("best score: ",accuracy_score(pr_lbl,tru_lbl))

        #print("f1_pos", f1(tp, fn, fp, tn), f1_neg)
        #print("f1_neg", f1(tp, fn, fp, tn), f1_neg)

        return loss, n_correct / n_test,f1_neg  #, f1_neg

    def run(self):
        train_dataloader, test_dataloader, valid_dataloader = load_data(dataset=dataset,
                                                      data_dir=data_dir,
                                                      tokenizer=self.tokenizer,
                                                      train_batch_size=train_batch_size,
                                                      test_batch_size=test_batch_size,
                                                      model_name=model_name,
                                                      method=method,
                                                      workers=0)
        _params = filter(lambda p: p.requires_grad, self.model.parameters())

        criterion = SupConLoss(alpha, temp)

        optimizer = torch.optim.AdamW(_params, lr=lr, weight_decay=decay)

        best_f1_p = 0
        best_f1_n = 0
        best_loss = float('inf')
        store_epo = 0

        for epoch in range(num_epoch):
            train_loss, train_acc = self._train(train_dataloader, criterion, optimizer)
            test_loss_t, test_acc_t, f1_pos_t = self._test(test_dataloader, criterion)
            test_loss, test_acc, f1_pos = self._test(valid_dataloader, criterion)

            if f1_pos > best_f1_p:  #f1_neg >= best_f1_n:

              #best_f1_n = f1_neg
              best_f1_p = f1_pos

              store_epo = epoch+1

              print("best F1-score", f1_pos)
              print("best epoch", store_epo)



              torch.save({'epoch': epoch,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'loss': test_loss
                            }, PATH+'{}_SCL_LR_model_iter.pth'.format(dataset) )

            self.logger.info('{}/{} - {:.2f}%'.format(epoch+1, num_epoch, 100*(epoch+1)/num_epoch))
            self.logger.info('[train] loss: {:.4f}, acc: {:.2f}'.format(train_loss, train_acc*100))
            self.logger.info('[test] loss: {:.4f}, acc: {:.2f}'.format(test_loss, test_acc*100))

        self.logger.info('best loss: {:.4f}, best acc: {:.2f}, store_epoch:{}'.format(best_loss, best_f1_p,store_epo))
        self.logger.info('log saved: {}'.format(log_name))

"""#MAIN FUNCTION"""

from sklearn.model_selection import train_test_split

num_epoch=25
promise_dim=23

dpccl = Instructor(logger)
dpccl.run()



"""# Result Analysis"""

class_labels=["Clean","Bug"]

"""Accuracy Score"""

def accuracy(y_test, predictions):
  RF_accuracy = accuracy_score(y_true=y_test, y_pred= predictions)
  return RF_accuracy

"""Classification Report"""

def class_report(y_test, predictions):
  print(classification_report(y_true=y_test, y_pred=predictions,target_names=class_labels))

"""Confusion Matrix"""

from mlxtend.plotting import plot_confusion_matrix
def confusion_mat(y_test, predictions):
  ax=plot_confusion_matrix(conf_mat=confusion_matrix(y_test, predictions),
                        figsize=(10,5),
                        class_names=class_labels,
                        cmap=plt.cm.Greens)
  plt.title(label="Confusion Matrix")
  plt.show()
  tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()
  return tn, fp, fn, tp

"""$Accuracy$ =  $\frac{TP + TN}{TP + TN + FP + FN}$"""

def accuracy(tn, fp, fn, tp):
  return (tp + tn) / (tp + tn + fp + fn)

"""$Recall$ = $Sensitivity$ = $True$ $Positive$ $Rate$ = $\frac{TP}{TP + FN}$"""

def tpr(tn, fp, fn, tp):
  return tp / (tp + fn)

"""$False$ $Negative$ $Rate$ = $\frac{FN}{TP + FN}$"""

def fnr(tn, fp, fn, tp):
  return fn / (tp + fn)

"""$Specificity$ = $True$ $Negative$ $Rate$ = $\frac{TN}{TN + FP}$"""

def tnr(tn, fp, fn, tp):
  return tn / (tn + fp)

"""$False$ $Positive$ $Rate$ = $\frac{FP}{TN + FP}$"""

def fpr(tn, fp, fn, tp):
  return fp / (tn + fp)

"""$Precision$ = $Positive$ $Predictive$ $Value$ = $\frac{TP}{TP + FP}$"""

def ppv(tn, fp, fn, tp):
  return tp / (tp + fp)

"""$Negative$ $Predictive$ $Value$ = $\frac{TN}{TN + FN}$"""

def npv(tn, fp, fn, tp):
  return tn / (tn + fn)

"""$Balanced$ $Accuracy$ = $\frac{(TPR + TNR)}{2}$"""

def balanced_accuracy(tn, fp, fn, tp):
  tprr = tpr(tn, fp, fn, tp)
  tnrr = tnr(tn, fp, fn, tp)
  return (tprr + tnrr) / 2

"""$F_1$ = $\frac{2*Precisicion*Recall}{Precision + Recall}$"""

def f1(tn, fp, fn, tp):
  p = ppv(tn, fp, fn, tp)
  r = tpr(tn, fp, fn, tp)

  return (2*p*r) / (p + r)

"""$Matthews$ $Correlation$ $Coefficient$ = $\frac{(TP * TN) - (FP * FN)}{\sqrt{(TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)}}$"""

from math import sqrt

def mcc(tn, fp, fn, tp):
  top = (tp * tn) - (fp * fn)
  bot = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)
  return top / sqrt(bot)

"""AUC"""

from sklearn import metrics
def auc(y_test, predictions):
  fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=1)
  return metrics.auc(fpr, tpr)

"""#load saved optimized model"""

PATH_Result= '/content/drive/MyDrive/Proposed_BaseLine/BERT_SCL_HC_LR/Results/'

tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base', add_prefix_space=True)
base_model= AutoModel.from_pretrained('microsoft/codebert-base')

import pandas as pd

def predict_defective_files_in_releases(dataset_name):
  model = Transformer(base_model, num_classes, method,promise_dim)
  checkpoint = torch.load(PATH+'{}_SCL_LR_model_iter.pth'.format(dataset_name))
  model.load_state_dict(checkpoint['model_state_dict'])
  model.to(device)
  train_dataloader, test_dataloader, valid_dataloader = load_data(dataset=dataset_name,
                                                      data_dir=data_dir,
                                                      tokenizer=tokenizer,
                                                      train_batch_size=train_batch_size,
                                                      test_batch_size=test_batch_size,
                                                      model_name=model_name,
                                                      method=method,
                                                      workers=0)
  criterion = SupConLoss(alpha, temp)
  _params = filter(lambda p: p.requires_grad, model.parameters())
  optimizer = torch.optim.AdamW(_params, lr=lr, weight_decay=decay)
  #####
  test_loss, n_correct, n_test = 0, 0, 0
  ###
  model.eval()
  n=0
  tru_lbl=[]
  pr_lbl=[]
  pr_prob=[]
  with torch.no_grad():
    for inputs, promise, targets in tqdm(test_dataloader, disable=backend, ascii=' >='):
      inputs = {k: v.to(device) for k, v in inputs.items()}
      promise = torch.tensor(promise).to(device)
      targets = targets.to(device)
      outputs = model(promise.float(), **inputs)
      loss = criterion(outputs, targets)
      test_loss += loss.item() * targets.size(0)
      n_correct += (torch.argmax(outputs['predicts'], -1) == targets).sum().item()
      n_test += targets.size(0)
      n +=1
      prediction=torch.argmax(outputs['predicts'],-1)
      pr_lbl.extend(prediction.detach().cpu().numpy())
      tru_lbl.extend(targets.detach().cpu().numpy())
      pr_prob.extend(torch.max(outputs['predicts'],1).values.detach().cpu().numpy())

  tn, fp, fn, tp = confusion_matrix(tru_lbl, pr_lbl).ravel()
  print(classification_report(tru_lbl, pr_lbl))
  print(confusion_matrix(tru_lbl, pr_lbl))

  path = PATH_Result+'Result_Proposed_SCL_LR.csv'
  if os.path.exists(path):
    df=pd.read_csv(path)
  else:
    # create an Empty DataFrame object
    df = pd.DataFrame(columns = ['Train Project', 'Test Project', 'True Positive',
                  'False Positive', 'True Negative', 'False Negative', 'F1-Score', 'Precision', 'Recall', 'MCC', 'AUC'])

  df = df.append({'Train Project':dataset_name,
                'Test Project':dataset_name,
                'True Positive':tp,
                'False Positive':fp,
                'True Negative':tn,
                'False Negative':fn,
                'Loss': test_loss,
                'F1-Score':f1(tn, fp, fn, tp),
                'Precision':ppv(tn, fp, fn, tp),
                'Recall':tpr(tn, fp, fn, tp),
                'MCC':mcc(tn, fp, fn, tp),
                'AUC':auc(tru_lbl, pr_lbl)},
                ignore_index = True)
  df.to_csv(path, index=False)
  df = pd.DataFrame(list(zip(tru_lbl, pr_prob, np.round_(np.array(pr_lbl)))),
               columns =['file_label', 'prediction-prob','prediction-label'])
  df.to_csv(PATH_Result+'/prediction/Test_{}_True_Predicted_Labels.csv'.format(dataset_name), index=False)
  print('finished',dataset_name)

"""#results"""

predict_defective_files_in_releases('velocity_1.5-1.6')

